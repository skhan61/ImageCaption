{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.getcwd()  # Check current working directory\n",
    "os.chdir('/home/sayem/Desktop/ImageCaption/notebooks')  # Change if necessary\n",
    "sys.path.append('../src')  # Now append the src path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# # Add the src directory to the Python path\n",
    "# sys.path.append('../src')\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "# Set the maximum number of rows to 100 (default is 10 in newer versions)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "# Set the maximum number of columns to 50 (default is 20)\n",
    "pd.set_option('display.max_columns', 1500)\n",
    "\n",
    "coco_data_folder = Path(\"/media/sayem/510B93E12554BBD1/CocoData\")\n",
    "\n",
    "# Define your device as either 'cuda' for GPU or 'cpu' for CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset import CocoCaptionsDataset, collate_fn, Vocabulary\n",
    "\n",
    "# # Image transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(256),  # Resize the image to a larger size while maintaining aspect ratio\n",
    "#     transforms.CenterCrop(224),  # Center crop the image to 224x224\n",
    "#     transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "#     transforms.ColorJitter(brightness=0.1, contrast=0.1, \\\n",
    "#         saturation=0.1, hue=0.1),  # Random color adjustments\n",
    "#     transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "#     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize with mean and std for pretrained models\n",
    "# ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image\n",
    "    transforms.CenterCrop(224),  # Center crop the image\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(degrees=90),  # Rotate the image by up to Â±30 degrees\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Random color adjustments\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize for pretrained models\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the NLTK data directory and download 'punkt' tokenizer model\n",
    "nltk_data_path = Path(\"/home/sayem/Desktop/ImageCaption/nltk_data\")\n",
    "nltk_data_path.mkdir(parents=True, exist_ok=True)\n",
    "nltk.data.path.append(str(nltk_data_path))\n",
    "nltk.download('punkt', download_dir=str(nltk_data_path))\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=str(nltk_data_path))\n",
    "\n",
    "\n",
    "# # Download the 'punkt' tokenizer models\n",
    "# nltk.download('punkt', download_dir=nltk_data_path)\n",
    "\n",
    "def build_vocab(json_file, threshold=5):\n",
    "    coco = COCO(json_file)\n",
    "    counter = Counter()\n",
    "\n",
    "    for i, id in enumerate(coco.anns.keys()):\n",
    "        caption = str(coco.anns[id]['caption']).lower()\n",
    "        tokens = word_tokenize(caption)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = Vocabulary()\n",
    "\n",
    "    for word, count in counter.items():\n",
    "        if count >= threshold:\n",
    "            vocab.add_word(word)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "# Paths\n",
    "coco_data_folder = Path(\"/media/sayem/510B93E12554BBD1/CocoData\")\n",
    "annotations_file = coco_data_folder / 'annotations' / 'captions_train2017.json'\n",
    "\n",
    "data_folder = Path('/home/sayem/Desktop/ImageCaption/data')\n",
    "data_folder.mkdir(parents=True, exist_ok=True)  # Ensure the directory exists\n",
    "vocab_file = data_folder / 'vocabulary.pkl'\n",
    "\n",
    "# Check if vocabulary file exists\n",
    "if not vocab_file.exists():\n",
    "    # Build the vocabulary from the annotations file\n",
    "    vocab = build_vocab(str(annotations_file))\n",
    "    \n",
    "    # Save the vocabulary\n",
    "    try:\n",
    "        with vocab_file.open('wb') as f:\n",
    "            pickle.dump(vocab, f)\n",
    "        print(\"Saved vocabulary to file.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "else:\n",
    "    # Load the existing vocabulary\n",
    "    try:\n",
    "        with vocab_file.open('rb') as f:\n",
    "            vocab = pickle.load(f)\n",
    "        print(\"Loaded vocabulary from file.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct way to join paths using pathlib\n",
    "train_dataset = CocoCaptionsDataset(\n",
    "    root_dir=coco_data_folder / 'train2017',\n",
    "    ann_file=coco_data_folder / 'annotations' / 'captions_train2017.json',\n",
    "    vocab=vocab,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Rest of your code remains the same\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=10  # Adjust this number based on your system's capabilities\n",
    ")\n",
    "\n",
    "# from torch.utils.data import Subset\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming train_dataset is already created with CocoCaptionsDataset\n",
    "# total_samples = len(train_dataset)  # Total number of samples in the dataset\n",
    "# subset_size = 10000  # Number of samples you want in the subset\n",
    "\n",
    "# # Randomly select 10,000 indices from the dataset\n",
    "# subset_indices = np.random.choice(total_samples, subset_size, replace=False)\n",
    "\n",
    "# # Create a subset dataset with these indices\n",
    "# subset_train_dataset = Subset(train_dataset, subset_indices)\n",
    "\n",
    "# # Use this subset for your DataLoader\n",
    "# train_loader = DataLoader(\n",
    "#     subset_train_dataset, \n",
    "#     batch_size=32, \n",
    "#     shuffle=True, \n",
    "#     collate_fn=collate_fn,\n",
    "#     num_workers=2  # Adjust this number based on your system's capabilities\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation dataset\n",
    "val_dataset = CocoCaptionsDataset(\n",
    "    root_dir=coco_data_folder / 'val2017',  # Path to validation images\n",
    "    ann_file=coco_data_folder / 'annotations' / 'captions_val2017.json',  # Path to validation annotations\n",
    "    vocab=vocab,  # Vocabulary instance\n",
    "    transform=transform  # Image transformations\n",
    ")\n",
    "\n",
    "# Create the DataLoader for the validation dataset\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=32,  # You can adjust the batch size if needed\n",
    "    shuffle=False,  # Usually, we don't need to shuffle the validation data\n",
    "    collate_fn=collate_fn,  # Use the same collate function as for the training dataset\n",
    "    num_workers=10\n",
    ")\n",
    "\n",
    "# # Assuming train_dataset is already created with CocoCaptionsDataset\n",
    "# total_samples = len(val_dataset)  # Total number of samples in the dataset\n",
    "# subset_size = 10000  # Number of samples you want in the subset\n",
    "\n",
    "# # Randomly select 10,000 indices from the dataset\n",
    "# subset_indices = np.random.choice(total_samples, subset_size, replace=False)\n",
    "\n",
    "# # Create a subset dataset with these indices\n",
    "# subset_val_dataset = Subset(val_dataset, subset_indices)\n",
    "\n",
    "# # Use this subset for your DataLoader\n",
    "# # Create the DataLoader for the validation dataset\n",
    "# val_loader = DataLoader(\n",
    "#     dataset=subset_val_dataset, \n",
    "#     batch_size=32,  # You can adjust the batch size if needed\n",
    "#     shuffle=False,  # Usually, we don't need to shuffle the validation data\n",
    "#     collate_fn=collate_fn,  # Use the same collate function as for the training dataset\n",
    "#     num_workers=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming val_loader is already defined\n",
    "\n",
    "# Function to convert a tensor to an image\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()  # Use plt.show() to display the image\n",
    "\n",
    "# Get a batch of data\n",
    "images, _, _, _ = next(iter(val_loader))\n",
    "\n",
    "# Select one image from the batch\n",
    "img = images[0]  # Select the first image in the batch\n",
    "\n",
    "# Display the image\n",
    "imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "# Get all captions for a specific image ID\n",
    "image_id = 344614 # Replace with an actual image ID\n",
    "captions = val_dataset.get_all_captions_for_image(image_id)\n",
    "image_path = val_dataset.get_image_path(image_id)\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Plotting\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Captions for image_id {image_id}\")\n",
    "plt.show()\n",
    "\n",
    "# Print the captions\n",
    "for i, caption in enumerate(captions, 1):\n",
    "    print(f\"Caption {i}: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, caption, length, image_id = next(iter(train_loader))\n",
    "\n",
    "print(img.shape)\n",
    "print(caption.shape)\n",
    "print(length.shape)\n",
    "print(image_id.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Specify your custom directory for TensorBoard logs\n",
    "log_dir = Path(\"/home/sayem/Desktop/ImageCaption/log_dir\")\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a TensorBoardLogger instance with the specified log directory\n",
    "logger = TensorBoardLogger(save_dir=log_dir, name=\"ImageCaptioning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from trainer.trainer import ImageCaptioningModel\n",
    "\n",
    "from models.decoders import DecoderLSTM\n",
    "from models.encoders import EncoderCNN  # Corrected typo in import\n",
    "\n",
    "# Define your parameters\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)  # Ensure 'vocab' is defined\n",
    "num_layers = 2\n",
    "max_seq_len = 20\n",
    "\n",
    "# Instantiate encoder and decoder\n",
    "encoder = EncoderCNN(embedding_size)\n",
    "decoder = DecoderLSTM(embedding_size, hidden_size, vocab_size, num_layers, max_seq_len)\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.00,\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Instantiate the ImageCaptioningModel with encoder and decoder\n",
    "model = ImageCaptioningModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    vocab=vocab, \n",
    "    val_dataset=val_dataset,  # Ensure 'val_dataset' is defined\n",
    "    optimizer_name='adam', \n",
    "    scheduler_name='plateau',\n",
    "    optimizer_params={'lr': 0.001},\n",
    "    scheduler_params={'mode': 'min', 'factor': 0.1, 'patience': 5}\n",
    ")\n",
    "\n",
    "# Assuming 'logger' is defined and 'train_loader' and 'val_loader' are available\n",
    "trainer = Trainer(logger=logger, max_epochs=15, callbacks=[early_stop_callback])\n",
    "\n",
    "# Start training\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Old\n",
    "\n",
    "# from pytorch_lightning import Trainer\n",
    "# from pytorch_lightning.callbacks import EarlyStopping\n",
    "# from trainer.trainer import ImageCaptioningModel\n",
    "\n",
    "# # EarlyStopping callback\n",
    "# early_stop_callback = EarlyStopping(\n",
    "#    monitor='val_loss',\n",
    "#    min_delta=0.00,\n",
    "#    patience=3,\n",
    "#    verbose=False,\n",
    "#    mode='min'\n",
    "# )\n",
    "\n",
    "# # Instantiate the model (assuming the model is already defined)\n",
    "# model = ImageCaptioningModel(\n",
    "#     embedding_size=256, \n",
    "#     hidden_size=512, \n",
    "#     vocab_size=len(vocab),\n",
    "#     vocab=vocab, \n",
    "#     num_layers=2, \n",
    "#     max_seq_len=20,\n",
    "#     optimizer_name='adam', \n",
    "#     scheduler_name='plateau',\n",
    "#     optimizer_params={'lr': 0.001},\n",
    "#     scheduler_params={'mode': 'min', 'factor': 0.1, 'patience': 5},\n",
    "#     val_dataset=val_dataset\n",
    "# )\n",
    "\n",
    "# # Create a Trainer instance with the custom logger\n",
    "# trainer = Trainer(logger=logger, max_epochs=15, callbacks=[early_stop_callback])\n",
    "\n",
    "# # Start training\n",
    "# trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and file path for saving the model\n",
    "model_save_dir = Path(\"/home/sayem/Desktop/ImageCaption/trained_models\")\n",
    "model_save_dir.mkdir(parents=True, exist_ok=True)  # Create the directory if it does not exist\n",
    "model_path = model_save_dir / \"complete_image_captioning_model.pth\"\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(model, model_path)\n",
    "\n",
    "print(f\"Complete model saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path where the model is saved\n",
    "model_path = Path(\"/home/sayem/Desktop/ImageCaption/trained_models/complete_image_captioning_model.pth\")\n",
    "\n",
    "# Check if the model file exists\n",
    "if model_path.is_file():\n",
    "    # Load the entire model\n",
    "    model = torch.load(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # Move the model to the appropriate device (if using GPU)\n",
    "    model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Now you can use the model for inference\n",
    "    # Example: model(input_tensor)\n",
    "else:\n",
    "    print(f\"Model file not found at {model_path}\")\n",
    "\n",
    "# [Rest of your code for using the model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directory containing test images\n",
    "coco_data_folder = Path(\"/media/sayem/510B93E12554BBD1/CocoData\")\n",
    "test_images_dir = coco_data_folder / 'test2017'\n",
    "\n",
    "# List all image files in the directory\n",
    "test_image_files = [f for f in test_images_dir.iterdir() if f.is_file()]\n",
    "\n",
    "# Select a random image file\n",
    "random_image_file = random.choice(test_image_files)\n",
    "\n",
    "# Load and display the image\n",
    "image = Image.open(random_image_file)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Turn off axis numbers\n",
    "plt.show()\n",
    "\n",
    "print(f\"Randomly selected image: {random_image_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "# Prepare an image\n",
    "image = load_image(random_image_file, transform)\n",
    "image_tensor = image.to(device)\n",
    "\n",
    "# Generate caption\n",
    "with torch.no_grad():\n",
    "    features = model.encoder(image_tensor)\n",
    "    sampled_ids = model.decoder.sample(features)\n",
    "    sampled_ids = sampled_ids[0].cpu().numpy()  # Convert to numpy array\n",
    "\n",
    "# Convert word_ids to words\n",
    "caption = []\n",
    "for word_id in sampled_ids:\n",
    "    word = vocab.idx2word[word_id]\n",
    "    caption.append(word)\n",
    "    if word == '<end>':\n",
    "        break\n",
    "caption = ' '.join(caption)\n",
    "\n",
    "# Print out the image and the generated caption\n",
    "print(caption)\n",
    "plt.imshow(Image.open(random_image_file))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipcv_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
