{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.getcwd()  # Check current working directory\n",
    "os.chdir('/home/sayem/Desktop/ImageCaption/notebooks')  # Change if necessary\n",
    "sys.path.append('../src')  # Now append the src path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# # Add the src directory to the Python path\n",
    "# sys.path.append('../src')\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "# Set the maximum number of rows to 100 (default is 10 in newer versions)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "# Set the maximum number of columns to 50 (default is 20)\n",
    "pd.set_option('display.max_columns', 1500)\n",
    "\n",
    "data_folder = \"/media/sayem/510B93E12554BBD1/CocoData\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset import CocoCaptionsDataset, collate_fn, Vocabulary\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image to a larger size while maintaining aspect ratio\n",
    "    transforms.CenterCrop(224),  # Center crop the image to 224x224\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, \\\n",
    "        saturation=0.1, hue=0.1),  # Random color adjustments\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize with mean and std for pretrained models\n",
    "])\n",
    "\n",
    "# # Enhanced image transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(256),  # Resize the image to a larger size\n",
    "#     transforms.RandomCrop(224),  # Randomly crop the image to 224x224\n",
    "#     transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "#     transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),  # Randomly change color properties\n",
    "#     transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "#     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize with mean and std for pretrained models\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sayem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Set the NLTK data directory\n",
    "nltk_data_path = \"/home/sayem/nltk_data\"\n",
    "os.makedirs(nltk_data_path, exist_ok=True)\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Download the 'punkt' tokenizer models\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "\n",
    "def build_vocab(json_file, threshold=5):\n",
    "    coco = COCO(json_file)\n",
    "    counter = Counter()\n",
    "\n",
    "    for i, id in enumerate(coco.anns.keys()):\n",
    "        caption = str(coco.anns[id]['caption']).lower()\n",
    "        tokens = word_tokenize(caption)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = Vocabulary()\n",
    "\n",
    "    for word, count in counter.items():\n",
    "        if count >= threshold:\n",
    "            vocab.add_word(word)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = build_vocab(os.path.join(data_folder, 'annotations', 'captions_train2017.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.41s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CocoCaptionsDataset(root_dir=data_folder+'/train2017',\n",
    "                                    ann_file=data_folder+'/annotations/captions_train2017.json',\n",
    "                                    vocab=vocab,\n",
    "                                    transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Create the validation dataset\n",
    "val_dataset = CocoCaptionsDataset(\n",
    "    root_dir=data_folder + '/val2017',  # Path to validation images\n",
    "    ann_file=data_folder + '/annotations/captions_val2017.json',  # Path to validation annotations\n",
    "    vocab=vocab,  # Vocabulary instance\n",
    "    transform=transform  # Image transformations\n",
    ")\n",
    "\n",
    "# Create the DataLoader for the validation dataset\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=32,  # You can adjust the batch size if needed\n",
    "    shuffle=False,  # Usually, we don't need to shuffle the validation data\n",
    "    collate_fn=collate_fn  # Use the same collate function as for the training dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 19])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "img, caption, length = next(iter(val_loader))\n",
    "\n",
    "print(img.shape)\n",
    "print(caption.shape)\n",
    "print(length.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.enocders import EncoderCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output features size: torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input tensor of size [batch_size, channels, width, height]\n",
    "dummy_input = torch.randn(32, 3, 224, 224)  # Batch size of 32, 3 color channels, 224x224 image size\n",
    "\n",
    "# Instantiate the EncoderCNN\n",
    "embed_size = 256  # Example embedding size\n",
    "encoder = EncoderCNN(embed_size)\n",
    "\n",
    "# Pass the dummy input through the encoder\n",
    "output_features = encoder(dummy_input)\n",
    "\n",
    "print(\"Output features size:\", output_features.size())  # Should be [32, embed_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipcv_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
